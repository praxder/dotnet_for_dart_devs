---
title: "Dapper: Performance and Bulk Operations"
day: 102
week: 21
module: 8
moduleName: "Dapper"
phase: "dotnet"
dartConcept: "performance in sqflite, batch inserts in Dart"
csharpConcept: "buffered vs unbuffered, async streaming, bulk insert, SqlBulkCopy, benchmarking"
estimatedMinutes: 30
isProject: false
---

import ConceptCallout from '../../../components/lesson/ConceptCallout.astro';
import DartEquivalent from '../../../components/lesson/DartEquivalent.astro';
import CodeComparison from '../../../components/lesson/CodeComparison.astro';
import ExerciseBlock from '../../../components/lesson/ExerciseBlock.astro';

Dapper is already very fast — near ADO.NET performance for single-row operations. But for bulk operations, large result sets, or high-concurrency APIs, there are specific techniques that make a significant difference.

## Buffered vs Unbuffered Queries

```csharp
// BUFFERED (default): loads all rows into memory, then returns
// Good for: small result sets, when you need to iterate multiple times
var products = await conn.QueryAsync<Product>(
    "SELECT * FROM products WHERE is_active = 1");
// All rows are in memory — safe to use .Count(), iterate multiple times, etc.

// UNBUFFERED: streams rows one at a time — O(1) memory
// Good for: large result sets that you process row-by-row
var products = await conn.QueryAsync<Product>(
    "SELECT * FROM products WHERE is_active = 1",
    buffered: false);
// Returns IEnumerable<T> backed by a live DataReader
// WARNING: must iterate fully before disposing the connection!
// Don't use buffered: false with LINQ filtering that breaks early (.First(), .Take(), etc.)
```

## Streaming Large Results — IAsyncEnumerable

```csharp
// Process millions of rows without loading them all into memory
// Useful for: exports, reports, data migrations

public async IAsyncEnumerable<Order> StreamAllOrdersAsync(
    [EnumeratorCancellation] CancellationToken ct = default)
{
    using var conn = factory.CreateConnection();

    // Use QueryUnbufferedAsync for true streaming (Dapper 2.1+)
    var orders = conn.QueryUnbufferedAsync<Order>(
        "SELECT * FROM orders ORDER BY ordered_at",
        cancellationToken: ct);

    await foreach (var order in orders.WithCancellation(ct))
        yield return order;
}

// Minimal API streaming endpoint — sends rows as they arrive
app.MapGet("/api/orders/export", async (
    IOrderRepository repo,
    HttpResponse response,
    CancellationToken ct) =>
{
    response.ContentType = "text/csv";
    response.Headers.ContentDisposition = "attachment; filename=orders.csv";
    await response.WriteAsync("Id,CustomerId,Total,Status,OrderedAt\n", ct);

    await foreach (var order in repo.StreamAllOrdersAsync(ct))
    {
        await response.WriteAsync(
            $"{order.Id},{order.CustomerId},{order.Total},{order.Status},{order.OrderedAt:O}\n", ct);
    }
});
```

## Bulk Insert — Multiple Approaches

```csharp
// Approach 1: ExecuteAsync with a list (simple, works everywhere)
// Dapper runs INSERT once per item — N round trips
// OK for hundreds of rows; slow for thousands
var newProducts = GetProductsFromCsv();
await conn.ExecuteAsync(
    "INSERT INTO products (name, price, category_id) VALUES (@Name, @Price, @CategoryId)",
    newProducts);
// 1000 products = 1000 INSERT statements

// Approach 2: Multi-value INSERT (one statement, N value groups)
// Much faster — single round trip regardless of count
public async Task BulkInsertProductsAsync(IEnumerable<Product> products)
{
    using var conn = factory.CreateConnection();
    var list = products.ToList();
    const int batchSize = 500;  // SQLite limit: 999 params / columns-per-row

    foreach (var batch in list.Chunk(batchSize))
    {
        var parameters = new DynamicParameters();
        var valueRows = new List<string>();

        for (int i = 0; i < batch.Length; i++)
        {
            valueRows.Add($"(@Name{i}, @Price{i}, @CategoryId{i})");
            parameters.Add($"@Name{i}", batch[i].Name);
            parameters.Add($"@Price{i}", batch[i].Price);
            parameters.Add($"@CategoryId{i}", batch[i].CategoryId);
        }

        var sql = $"INSERT INTO products (name, price, category_id) VALUES {string.Join(", ", valueRows)}";
        await conn.ExecuteAsync(sql, parameters);
    }
}

// Approach 3: SqlBulkCopy — fastest for SQL Server (millions of rows)
public async Task SqlBulkInsertAsync(IEnumerable<Product> products, IDbConnection conn)
{
    var sqlConn = (SqlConnection)conn;
    await sqlConn.OpenAsync();

    using var bulkCopy = new SqlBulkCopy(sqlConn)
    {
        DestinationTableName = "products",
        BatchSize = 5000,
        BulkCopyTimeout = 120
    };
    bulkCopy.ColumnMappings.Add("Name", "name");
    bulkCopy.ColumnMappings.Add("Price", "price");
    bulkCopy.ColumnMappings.Add("CategoryId", "category_id");

    var table = new DataTable();
    table.Columns.Add("Name",       typeof(string));
    table.Columns.Add("Price",      typeof(decimal));
    table.Columns.Add("CategoryId", typeof(int));

    foreach (var p in products)
        table.Rows.Add(p.Name, p.Price, p.CategoryId);

    await bulkCopy.WriteToServerAsync(table);
}
// SqlBulkCopy: 50,000 rows → ~0.5 seconds vs ~30 seconds with individual INSERTs
```

## Bulk Update and Delete

```csharp
// Bulk update: build a single UPDATE with CASE WHEN
public async Task BulkUpdatePricesAsync(IEnumerable<(int Id, decimal Price)> updates)
{
    using var conn = factory.CreateConnection();
    var list = updates.ToList();

    if (!list.Any()) return;

    var parameters = new DynamicParameters();
    var cases = new List<string>();
    var ids = new List<int>();

    for (int i = 0; i < list.Count; i++)
    {
        cases.Add($"WHEN id = @Id{i} THEN @Price{i}");
        parameters.Add($"@Id{i}", list[i].Id);
        parameters.Add($"@Price{i}", list[i].Price);
        ids.Add(list[i].Id);
    }

    var sql = $@"
        UPDATE products
        SET price = CASE {string.Join(" ", cases)} END
        WHERE id IN ({string.Join(",", ids)})";

    await conn.ExecuteAsync(sql, parameters);
}

// Bulk delete (simple and fast):
public async Task BulkDeleteAsync(IEnumerable<int> ids)
{
    if (!ids.Any()) return;
    using var conn = factory.CreateConnection();
    await conn.ExecuteAsync(
        "DELETE FROM products WHERE id IN @Ids",
        new { Ids = ids });
}
```

## Async Best Practices

```csharp
// Always use async overloads — they don't block the thread pool
// ✅ Correct:
var products = await conn.QueryAsync<Product>("SELECT * FROM products");

// ❌ Avoid:
var products = conn.Query<Product>("SELECT * FROM products");  // synchronous = blocks thread

// Parallel queries — when results are independent:
public async Task<DashboardData> GetDashboardAsync()
{
    using var conn = factory.CreateConnection();

    // Run independent queries in parallel (same connection, different commands)
    // Note: SQLite doesn't support true parallel queries on one connection
    // For SQL Server / PostgreSQL: open separate connections for true parallelism
    var ordersTask   = conn.QueryAsync<OrderSummary>("SELECT ...");
    var productsTask = conn.QueryAsync<ProductSummary>("SELECT ...");
    var customersTask = conn.QueryAsync<CustomerSummary>("SELECT ...");

    await Task.WhenAll(ordersTask, productsTask, customersTask);

    return new DashboardData(
        await ordersTask,
        await productsTask,
        await customersTask);
}
```

## Benchmarking Dapper Queries

```csharp
// Use BenchmarkDotNet to measure actual performance differences
// dotnet add package BenchmarkDotNet

[MemoryDiagnoser]
public class DapperBenchmarks
{
    private IDbConnectionFactory _factory = null!;

    [GlobalSetup]
    public void Setup()
    {
        _factory = new SqliteConnectionFactory("Data Source=bench.db");
        // Seed 10,000 products
    }

    [Benchmark(Baseline = true)]
    public async Task<List<Product>> QueryAll()
    {
        using var conn = _factory.CreateConnection();
        return (await conn.QueryAsync<Product>("SELECT * FROM products")).ToList();
    }

    [Benchmark]
    public async Task<List<ProductDto>> QueryProjected()
    {
        using var conn = _factory.CreateConnection();
        return (await conn.QueryAsync<ProductDto>(
            "SELECT id, name, price FROM products")).ToList();
    }

    [Benchmark]
    public async Task<List<Product>> QueryUnbuffered()
    {
        using var conn = _factory.CreateConnection();
        var results = new List<Product>();
        await foreach (var p in conn.QueryUnbufferedAsync<Product>("SELECT * FROM products"))
            results.Add(p);
        return results;
    }
}
```

<ConceptCallout type="tip" title="Dapper Performance Profile">
Dapper is typically **5-10x faster than EF Core** for read operations (less mapping overhead, no change tracking) and roughly equal for writes. Its main performance advantage is predictability: the SQL you write is exactly what runs — no hidden N+1, no lazy loading surprises, no Cartesian explosion.
</ConceptCallout>

<ExerciseBlock>
1. Benchmark three approaches for inserting 10,000 products: (a) `ExecuteAsync` with a list (N individual INSERTs), (b) multi-value INSERT batched in groups of 500, (c) `SqlBulkCopy` (SQL Server) or equivalent. Record time and rows-per-second for each.
2. Implement a streaming CSV export endpoint for orders that can handle 1 million rows without exceeding 50MB RAM usage. Verify with `GC.GetTotalAllocatedBytes()` and `Process.GetCurrentProcess().WorkingSet64` that memory stays bounded during the export.
3. Write a `DataCleanupService` as a BackgroundService that runs nightly: deletes orders older than 5 years (`DELETE FROM orders WHERE ordered_at < @Cutoff`), archives audit logs older than 2 years to a separate table, and logs the row counts for each operation. Use a transaction so both operations are atomic.
</ExerciseBlock>
